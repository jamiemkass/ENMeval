doParallel::registerDoParallel(cl)
opts <- NULL
} else if(parallelType == "doSNOW") {
doSNOW::registerDoSNOW(cl)
if(quiet != TRUE) opts <- list(progress=progress) else opts <- NULL
}
numCoresUsed <- foreach::getDoParWorkers()
if(quiet != TRUE) message(paste0("\nOf ", allCores, " total cores using ", numCoresUsed, "..."))
if(quiet != TRUE) message(paste0("Running in parallel using ", parallelType, "..."))
results <- foreach::foreach(i = 1:n, .options.snow = opts, .export = "cv.enm") %dopar% {
cv.enm(d, envs, enm, partitions, tune.tbl[i,], other.settings, partition.settings, user.val.grps, occs.testing.z, user.eval, quiet)
}
if(quiet != TRUE) close(pb)
parallel::stopCluster(cl)
return(results)
}
#' @rdname tune.enm
tune.regular <- function(d, envs, enm, partitions, tune.tbl, other.settings, partition.settings, user.val.grps, occs.testing.z, updateProgress, user.eval, quiet) {
results <- list()
n <- ifelse(!is.null(tune.tbl), nrow(tune.tbl), 1)
# set up the console progress bar
if(quiet != TRUE) pb <- txtProgressBar(0, n, style = 3)
for(i in 1:n) {
# and (optionally) the shiny progress bar (updateProgress)
if(n > 1) {
if(is.function(updateProgress)) {
text <- paste0('Running ', paste(as.character(tune.tbl[i,]), collapse = ""), '...')
updateProgress(detail = text)
}
if(quiet != TRUE) setTxtProgressBar(pb, i)
}
# set the current tune settings
tune.tbl.i <- tune.tbl[i,]
results[[i]] <- cv.enm(d, envs, enm, partitions, tune.tbl.i, other.settings, partition.settings, user.val.grps, occs.testing.z, user.eval, quiet)
}
if(quiet != TRUE) close(pb)
return(results)
}
#' @rdname tune.enm
cv.enm <- function(d, envs, enm, partitions, tune.tbl.i, other.settings, partition.settings, user.val.grps, occs.testing.z, user.eval, quiet) {
envs.names <- names(d[, 3:(ncol(d)-2)])
# unpack predictor variable values for occs and bg
occs.xy <- d %>% dplyr::filter(pb == 1) %>% dplyr::select(1:2)
occs.z <- d %>% dplyr::filter(pb == 1) %>% dplyr::select(dplyr::all_of(envs.names))
bg.xy <- d %>% dplyr::filter(pb == 0) %>% dplyr::select(1:2)
bg.z <- d %>% dplyr::filter(pb == 0) %>% dplyr::select(dplyr::all_of(envs.names))
# define number of grp (the value of "k") for occurrences
nk <- length(unique(d[d$pb == 1, "grp"]))
# build the full model from all the data
# assign arguments
mod.full.args <- enm@args(occs.z, bg.z, tune.tbl.i, other.settings)
# run training model with specified arguments
mod.full <- do.call(enm@fun, mod.full.args)
if(is.null(mod.full)) stop('Training model is NULL. Consider changing the tuning parameters or inputting more background points.')
# make full model prediction as raster using raster envs (if raster envs exists)
# or full model prediction table using the occs and bg values (if raster envs does not exist)
if(!is.null(envs)) {
pred.envs <- envs
}else{
pred.envs <- d %>% dplyr::select(dplyr::all_of(envs.names))
}
mod.full.pred <- enm@predict(mod.full, pred.envs, tune.tbl.i, other.settings)
# get evaluation statistics for training data
train <- tune.train(enm, occs.z, bg.z, mod.full, envs, tune.tbl.i, other.settings, partitions, quiet)
# make training stats table
tune.args.col <- paste(names(tune.tbl.i), tune.tbl.i, collapse = "_", sep = ".")
train.stats.df <- data.frame(tune.args = tune.args.col, stringsAsFactors = FALSE) %>% cbind(train)
# if no partitions, return results without cv.stats
if(partitions == "none") {
cv.res <- list(mod.full = mod.full, mod.full.pred = mod.full.pred, train.stats = train.stats.df, cv.stats = NULL)
return(cv.res)
}
if(partitions == "testing") {
bg.val.z <- data.frame()
occs.testing.zEnvs <- occs.testing.z %>% dplyr::select(dplyr::all_of(envs.names))
if(other.settings$doClamp == TRUE) {
occs.testing.zEnvs <- clamp.vars(orig.vals = occs.testing.zEnvs, ref.vals = rbind(occs.z, bg.z),
left = other.settings$clamp.directions$left, right = other.settings$clamp.directions$right,
categoricals = other.settings$categoricals)
}
validate <- tune.validate(enm, occs.z, occs.testing.zEnvs, bg.z, bg.val.z, mod.full, 0, tune.tbl.i, other.settings, partitions, user.eval, quiet)
test.stats.df <- data.frame(tune.args = tune.args.col, fold = 0, stringsAsFactors = FALSE) %>% cbind(validate)
cv.res <- list(mod.full = mod.full, mod.full.pred = mod.full.pred, train.stats = train.stats.df, cv.stats = test.stats.df)
return(cv.res)
}
# list to contain cv statistics
cv.stats <- list()
for(k in 1:nk) {
# assign partitions for training and validation occurrence data and for background data
occs.train.z <- d %>% dplyr::filter(pb == 1, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.train.z <- d %>% dplyr::filter(pb == 0, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
if(is.null(user.val.grps)) {
occs.val.z <- d %>% dplyr::filter(pb == 1, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
}else{
# assign partitions for training and validation occurrence data and for background data based on user data
occs.val.z <- user.val.grps %>% dplyr::filter(grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(envs.names)
}
# if doClamp is on, make sure that the validation data for each validation model is also clamped
# this means for each partition, making sure no values in validation data are more extreme than those in training data
if(other.settings$doClamp == TRUE) {
val.z <- clamp.vars(orig.vals = rbind(occs.val.z, bg.val.z), ref.vals = rbind(occs.train.z, bg.train.z),
left = other.settings$clamp.directions$left, right = other.settings$clamp.directions$right,
categoricals = other.settings$categoricals)
occs.val.z <- val.z[1:nrow(occs.val.z),]
if(nrow(bg.val.z) > 0) bg.val.z <- val.z[(nrow(occs.val.z)+1):nrow(bg.val.z),]
}
# define model arguments for current model k
mod.k.args <- enm@args(occs.train.z, bg.train.z, tune.tbl.i, other.settings)
# run model k with specified arguments
mod.k <- tryCatch({
do.call(enm@fun, mod.k.args)
}, error = function(cond) {
if(quiet != TRUE) message(paste0("\n", cond, "\n"))
# Choose a return value in case of error
return(NULL)
})
# if model is NULL for some reason, continue but report to user
if(is.null(mod.k)) {
if(quiet != TRUE) message(paste0("\nThe model for settings ", paste(names(tune.tbl.i), tune.tbl.i, collapse = ", "), " for partition ",
k, " failed (resulted in NULL). Consider changing partitions. Cross validation averages will ignore this model."))
next
}
validate <- tune.validate(enm, occs.train.z, occs.val.z, bg.train.z, bg.val.z, mod.k, nk, tune.tbl.i, other.settings, partitions, user.eval, quiet)
# put into list as one-row data frame for easy binding
cv.stats[[k]] <- data.frame(tune.args = tune.args.col, fold = k, stringsAsFactors = FALSE) %>% cbind(validate)
}
cv.stats.df <- dplyr::bind_rows(cv.stats)
cv.res <- list(mod.full = mod.full, mod.full.pred = mod.full.pred, train.stats = train.stats.df, cv.stats = cv.stats.df)
return(cv.res)
}
# get evaluation statistics for training data
train <- tune.train(enm, occs.z, bg.z, mod.full, envs, tune.tbl.i, other.settings, partitions, quiet)
# make training stats table
tune.args.col <- paste(names(tune.tbl.i), tune.tbl.i, collapse = "_", sep = ".")
train.stats.df <- data.frame(tune.args = tune.args.col, stringsAsFactors = FALSE) %>% cbind(train)
# if no partitions, return results without cv.stats
if(partitions == "none") {
cv.res <- list(mod.full = mod.full, mod.full.pred = mod.full.pred, train.stats = train.stats.df, cv.stats = NULL)
return(cv.res)
}
if(partitions == "testing") {
bg.val.z <- data.frame()
occs.testing.zEnvs <- occs.testing.z %>% dplyr::select(dplyr::all_of(envs.names))
if(other.settings$doClamp == TRUE) {
occs.testing.zEnvs <- clamp.vars(orig.vals = occs.testing.zEnvs, ref.vals = rbind(occs.z, bg.z),
left = other.settings$clamp.directions$left, right = other.settings$clamp.directions$right,
categoricals = other.settings$categoricals)
}
validate <- tune.validate(enm, occs.z, occs.testing.zEnvs, bg.z, bg.val.z, mod.full, 0, tune.tbl.i, other.settings, partitions, user.eval, quiet)
test.stats.df <- data.frame(tune.args = tune.args.col, fold = 0, stringsAsFactors = FALSE) %>% cbind(validate)
cv.res <- list(mod.full = mod.full, mod.full.pred = mod.full.pred, train.stats = train.stats.df, cv.stats = test.stats.df)
return(cv.res)
}
# list to contain cv statistics
cv.stats <- list()
for(k in 1:nk) {
# assign partitions for training and validation occurrence data and for background data
occs.train.z <- d %>% dplyr::filter(pb == 1, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.train.z <- d %>% dplyr::filter(pb == 0, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
if(is.null(user.val.grps)) {
occs.val.z <- d %>% dplyr::filter(pb == 1, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
}else{
# assign partitions for training and validation occurrence data and for background data based on user data
occs.val.z <- user.val.grps %>% dplyr::filter(grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(envs.names)
}
# if doClamp is on, make sure that the validation data for each validation model is also clamped
# this means for each partition, making sure no values in validation data are more extreme than those in training data
if(other.settings$doClamp == TRUE) {
val.z <- clamp.vars(orig.vals = rbind(occs.val.z, bg.val.z), ref.vals = rbind(occs.train.z, bg.train.z),
left = other.settings$clamp.directions$left, right = other.settings$clamp.directions$right,
categoricals = other.settings$categoricals)
occs.val.z <- val.z[1:nrow(occs.val.z),]
if(nrow(bg.val.z) > 0) bg.val.z <- val.z[(nrow(occs.val.z)+1):nrow(bg.val.z),]
}
# define model arguments for current model k
mod.k.args <- enm@args(occs.train.z, bg.train.z, tune.tbl.i, other.settings)
# run model k with specified arguments
mod.k <- tryCatch({
do.call(enm@fun, mod.k.args)
}, error = function(cond) {
if(quiet != TRUE) message(paste0("\n", cond, "\n"))
# Choose a return value in case of error
return(NULL)
})
# if model is NULL for some reason, continue but report to user
if(is.null(mod.k)) {
if(quiet != TRUE) message(paste0("\nThe model for settings ", paste(names(tune.tbl.i), tune.tbl.i, collapse = ", "), " for partition ",
k, " failed (resulted in NULL). Consider changing partitions. Cross validation averages will ignore this model."))
next
}
validate <- tune.validate(enm, occs.train.z, occs.val.z, bg.train.z, bg.val.z, mod.k, nk, tune.tbl.i, other.settings, partitions, user.eval, quiet)
# put into list as one-row data frame for easy binding
cv.stats[[k]] <- data.frame(tune.args = tune.args.col, fold = k, stringsAsFactors = FALSE) %>% cbind(validate)
}
k=1
# assign partitions for training and validation occurrence data and for background data
occs.train.z <- d %>% dplyr::filter(pb == 1, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.train.z <- d %>% dplyr::filter(pb == 0, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
if(is.null(user.val.grps)) {
occs.val.z <- d %>% dplyr::filter(pb == 1, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
}else{
# assign partitions for training and validation occurrence data and for background data based on user data
occs.val.z <- user.val.grps %>% dplyr::filter(grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(envs.names)
}
# if doClamp is on, make sure that the validation data for each validation model is also clamped
# this means for each partition, making sure no values in validation data are more extreme than those in training data
if(other.settings$doClamp == TRUE) {
val.z <- clamp.vars(orig.vals = rbind(occs.val.z, bg.val.z), ref.vals = rbind(occs.train.z, bg.train.z),
left = other.settings$clamp.directions$left, right = other.settings$clamp.directions$right,
categoricals = other.settings$categoricals)
occs.val.z <- val.z[1:nrow(occs.val.z),]
if(nrow(bg.val.z) > 0) bg.val.z <- val.z[(nrow(occs.val.z)+1):nrow(bg.val.z),]
}
# define model arguments for current model k
mod.k.args <- enm@args(occs.train.z, bg.train.z, tune.tbl.i, other.settings)
# run model k with specified arguments
mod.k <- tryCatch({
do.call(enm@fun, mod.k.args)
}, error = function(cond) {
if(quiet != TRUE) message(paste0("\n", cond, "\n"))
# Choose a return value in case of error
return(NULL)
})
mod.k.args
k
# run model k with specified arguments
mod.k <- tryCatch({
do.call(enm@fun, mod.k.args)
}, error = function(cond) {
if(quiet != TRUE) message(paste0("\n", cond, "\n"))
# Choose a return value in case of error
return(NULL)
})
# if model is NULL for some reason, continue but report to user
if(is.null(mod.k)) {
if(quiet != TRUE) message(paste0("\nThe model for settings ", paste(names(tune.tbl.i), tune.tbl.i, collapse = ", "), " for partition ",
k, " failed (resulted in NULL). Consider changing partitions. Cross validation averages will ignore this model."))
next
}
validate <- tune.validate(enm, occs.train.z, occs.val.z, bg.train.z, bg.val.z, mod.k, nk, tune.tbl.i, other.settings, partitions, user.eval, quiet)
# get model predictions for training and validation data for partition k
occs.train.pred <- enm@predict(mod.k, occs.train.z, tune.tbl.i, other.settings)
occs.val.pred <- enm@predict(mod.k, occs.val.z, tune.tbl.i, other.settings)
bg.train.pred <- enm@predict(mod.k, bg.train.z, tune.tbl.i, other.settings)
# if the background was partitioned, get predictions for this subset; if not, it will be NULL
if(nrow(bg.val.z) > 0) {
bg.val.pred <- enm@predict(mod.k, bg.val.z, tune.tbl.i, other.settings)
}else{
bg.val.pred <- NULL
# if(quiet != TRUE) message("\nNOTE: Background points were not partitioned for this analysis, so model validation will proceed on the full background.")
}
# if validation.bg == "full", calculate training and validation AUC and CBI based on the full background
# (training + validation background for all statistics)
# see Radosavljevic & Anderson 2014 for an example of calculating validation AUC with spatial partitions over a shared background
if(other.settings$validation.bg == "full") {
e.train <- dismo::evaluate(occs.train.pred, c(bg.train.pred, bg.val.pred))
auc.train <- e.train@auc
e.val <- dismo::evaluate(occs.val.pred, c(bg.train.pred, bg.val.pred))
auc.val <- e.val@auc
# calculate AUC diff as training AUC minus validation AUC with a shared background
auc.diff <- auc.train - auc.val
# calculate CBI based on the full background (do not calculate for jackknife partitions)
if(partitions != "jackknife") {
cbi.val <- ecospat::ecospat.boyce(c(bg.train.pred, bg.val.pred, occs.val.pred), occs.val.pred, PEplot = FALSE)$Spearman.cor
}else{
cbi.val <- NA
}
# if validation.bg == "partition", calculate training and validation AUC and CBI based on the partitioned backgrounds only
# (training background for training statistics and validation background for validation statistics)
}else if(other.settings$validation.bg == "partition") {
e.train <- dismo::evaluate(occs.train.pred, bg.train.pred)
auc.train <- e.train@auc
e.val <- dismo::evaluate(occs.val.pred, bg.val.pred)
auc.val <- e.val@auc
# calculate AUC diff as training AUC minus validation AUC with different backgrounds
auc.diff <- auc.train - auc.val
# calculate CBI based on the validation background only (do not calculate for jackknife partitions)
if(partitions != "jackknife") {
cbi.val <- ecospat::ecospat.boyce(c(bg.val.pred, occs.val.pred), occs.val.pred, PEplot = FALSE)$Spearman.cor
}else{
cbi.val <- NA
}
}
other.settings$validation.bg
e.train <- dismo::evaluate(occs.train.pred, c(bg.train.pred, bg.val.pred))
occs.train.pred
hist(occs.train.pred)
tune.tbl.i
mod.k
other.settings
enm@predict(mod.k, occs.train.z, tune.tbl.i, other.settings)
dismo::response(mod.k)
mod.k@lambdas
suit_1_test_enneval
suit_1_test_enneval@results
suit_1_test_enneval@results %>% tail
suit_1_test_enneval@results %>% tail(10)
suit_1_test_enneval@models[[196]]
dismo::response(suit_1_test_enneval@models[[196]])
e.train <- dismo::evaluate(occs.train.pred, c(bg.train.pred, bg.val.pred))
c(bg.train.pred, bg.val.pred)
k
# assign partitions for training and validation occurrence data and for background data
occs.train.z <- d %>% dplyr::filter(pb == 1, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.train.z <- d %>% dplyr::filter(pb == 0, grp != k) %>% dplyr::select(dplyr::all_of(envs.names))
if(is.null(user.val.grps)) {
occs.val.z <- d %>% dplyr::filter(pb == 1, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
}else{
# assign partitions for training and validation occurrence data and for background data based on user data
occs.val.z <- user.val.grps %>% dplyr::filter(grp == k) %>% dplyr::select(dplyr::all_of(envs.names))
bg.val.z <- d %>% dplyr::filter(pb == 0, grp == k) %>% dplyr::select(envs.names)
}
# if doClamp is on, make sure that the validation data for each validation model is also clamped
# this means for each partition, making sure no values in validation data are more extreme than those in training data
if(other.settings$doClamp == TRUE) {
val.z <- clamp.vars(orig.vals = rbind(occs.val.z, bg.val.z), ref.vals = rbind(occs.train.z, bg.train.z),
left = other.settings$clamp.directions$left, right = other.settings$clamp.directions$right,
categoricals = other.settings$categoricals)
occs.val.z <- val.z[1:nrow(occs.val.z),]
if(nrow(bg.val.z) > 0) bg.val.z <- val.z[(nrow(occs.val.z)+1):nrow(bg.val.z),]
}
# define model arguments for current model k
mod.k.args <- enm@args(occs.train.z, bg.train.z, tune.tbl.i, other.settings)
# run model k with specified arguments
mod.k <- tryCatch({
do.call(enm@fun, mod.k.args)
}, error = function(cond) {
if(quiet != TRUE) message(paste0("\n", cond, "\n"))
# Choose a return value in case of error
return(NULL)
})
dismo::response(mod.k)
library(ENMeval)
library(dismo)
library(dplyr)
library(ENMeval)
library(ENMeval)
library(dismo)
library(dplyr)
occs <- read.csv("/Users/kass/Documents/ms_archive/ENMeval_testing/Occdata_SWD.csv")
occs.xy <- occs[,2:3]
occs.vals <- occs[,4:22]
occs.grps <- read.csv("/Users/kass/Documents/ms_archive/ENMeval_testing/Occdata_Groups.csv")$Group
bg <- read.csv("/Users/kass/Documents/ms_archive/ENMeval_testing/background_SWD.csv")
bg.xy <- bg[,2:3]
bg.vals <- bg[,4:22]
bg.grps <- read.csv("/Users/kass/Documents/ms_archive/ENMeval_testing/background_Groups.csv")$group
results <- read.csv("/Users/kass/Documents/ms_archive/ENMeval_testing/Rcreper_1km_RESULTS.csv")
tune.args <- list(fc = as.character(unique(results$features)), rm = seq(0.5, 5, by = 0.5))
occs <- cbind(occs.xy, occs.vals)
names(occs)[1:2] <- c("x","y")
bg <- cbind(bg.xy, bg.vals)
user.grp <- list(occs.grp = occs.grps, bg.grp = bg.grps)
e <- ENMevaluate(occs, bg = bg, partitions = "user", tune.args=tune.args, algorithm = "maxent.jar", user.grp = user.grp, parallel = TRUE)
e
results %>% head()
results$rm <- factor(results$rm)
results_new %>% arrange(or.10p.avg) %>% head()
results_new <- eval.results(e) %>%
select(fc:rm, auc.train, auc.val.avg, auc.diff.avg, or.mtp.avg, or.10p.avg, ncoef) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "new.value") %>%
mutate(rm = as.numeric(as.character(rm)))
results_old <- results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
filter(rm > 0)
results
results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm)
results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value")
results_old <- results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
filter(as.numeric(rm) > 0)
results_old
res <- left_join(results_new, results_old, by = c("fc", "rm", "statistic")) %>%
mutate(type = ifelse(grepl("HPT", fc), "HPT",
ifelse(grepl("HP", fc), "HP",
ifelse(grepl("H", fc), "H",
ifelse(grepl("P", fc), "P", "L/Q")))),
statistic = factor(statistic, levels = c("auc.train", "auc.val.avg", "auc.diff.avg", "or.mtp.avg", "or.10p.avg", "ncoef")),
type = factor(type, levels = c("H", "P", "HP", "HPT", "L/Q")))
left_join(results_new, results_old, by = c("fc", "rm", "statistic"))
results_old
results_new
results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value")
results_old <- results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
mutate(rm = as.double(rm), fc = factor(fc)) %>%
filter(rm > 0)
res <- left_join(results_new, results_old, by = c("fc", "rm", "statistic")) %>%
mutate(type = ifelse(grepl("HPT", fc), "HPT",
ifelse(grepl("HP", fc), "HP",
ifelse(grepl("H", fc), "H",
ifelse(grepl("P", fc), "P", "L/Q")))),
statistic = factor(statistic, levels = c("auc.train", "auc.val.avg", "auc.diff.avg", "or.mtp.avg", "or.10p.avg", "ncoef")),
type = factor(type, levels = c("H", "P", "HP", "HPT", "L/Q")))
par(mfrow=c(2,3))
ggplot(res, aes(x=new.value, y=old.value, color=type)) + geom_point() +
xlab("maxent.jar ENMeval v1.0") + ylab("maxent.jar GUI SWD") +
geom_abline(intercept = 0, slope = 1) +
scale_color_brewer(palette = "Set1") +
facet_wrap(vars(statistic), scales = "free") +
theme_bw()
library(ggplot2)
ggplot(res, aes(x=new.value, y=old.value, color=type)) + geom_point() +
xlab("maxent.jar ENMeval v1.0") + ylab("maxent.jar GUI SWD") +
geom_abline(intercept = 0, slope = 1) +
scale_color_brewer(palette = "Set1") +
facet_wrap(vars(statistic), scales = "free") +
theme_bw()
user.grp
eval.results(e)
results
res
results_new
results_old
left_join(results_new, results_old, by = c("fc", "rm", "statistic"))
q=left_join(results_new, results_old, by = c("fc", "rm", "statistic"))
q$old.value
results_old
results
results %>% distinct(features, rm)
results %>% distinct(features, rm) %>% arrange(features, rm)
res
results_new
results_old
results_old %>% arrange(rm)
results_old <- results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
mutate(rm = as.double(rm), fc = factor(fc)) %>%
filter(rm != 0)
res <- left_join(results_new, results_old, by = c("fc", "rm", "statistic")) %>%
mutate(type = ifelse(grepl("HPT", fc), "HPT",
ifelse(grepl("HP", fc), "HP",
ifelse(grepl("H", fc), "H",
ifelse(grepl("P", fc), "P", "L/Q")))),
statistic = factor(statistic, levels = c("auc.train", "auc.val.avg", "auc.diff.avg", "or.mtp.avg", "or.10p.avg", "ncoef")),
type = factor(type, levels = c("H", "P", "HP", "HPT", "L/Q")))
res
results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
mutate(rm = as.double(rm), fc = factor(fc))
results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
mutate(rm = as.double(rm), fc = factor(fc)) %>% arrange(rm)
results
results$rm
as.double(results$rm)
as.numeric(results$rm)
as.double(as.character(results$rm))
results_old <- results %>%
select(fc = features, rm, auc.train = full.AUC, auc.val.avg = Mean.AUC, auc.diff.avg = Mean.AUC.DIFF, or.mtp.avg = Mean.ORmin, or.10p.avg = Mean.OR10, ncoef = nparm) %>%
tidyr::pivot_longer(cols = auc.train:ncoef, names_to = "statistic", values_to = "old.value") %>%
mutate(rm = as.double(as.character(rm)), fc = factor(fc)) %>%
filter(rm != 0)
res <- left_join(results_new, results_old, by = c("fc", "rm", "statistic")) %>%
mutate(type = ifelse(grepl("HPT", fc), "HPT",
ifelse(grepl("HP", fc), "HP",
ifelse(grepl("H", fc), "H",
ifelse(grepl("P", fc), "P", "L/Q")))),
statistic = factor(statistic, levels = c("auc.train", "auc.val.avg", "auc.diff.avg", "or.mtp.avg", "or.10p.avg", "ncoef")),
type = factor(type, levels = c("H", "P", "HP", "HPT", "L/Q")))
res
ggplot(res, aes(x=new.value, y=old.value, color=type)) + geom_point() +
xlab("maxent.jar ENMeval v1.0") + ylab("maxent.jar GUI SWD") +
geom_abline(intercept = 0, slope = 1) +
scale_color_brewer(palette = "Set1") +
facet_wrap(vars(statistic), scales = "free") +
theme_bw()
ggplot(res, aes(x=new.value, y=old.value, color=type)) + geom_point() +
xlab("maxent.jar ENMeval v2.0") + ylab("maxent.jar GUI SWD") +
geom_abline(intercept = 0, slope = 1) +
scale_color_brewer(palette = "Set1") +
facet_wrap(vars(statistic), scales = "free") +
theme_bw()
ggplot(res, aes(x=new.value, y=old.value, color=type)) + geom_point() +
xlab("maxent.jar ENMeval 3.4.3 v2.0") + ylab("R. creper Maxent 3.3.3k GUI SWD") +
geom_abline(intercept = 0, slope = 1) +
scale_color_brewer(palette = "Set1") +
facet_wrap(vars(statistic), scales = "free") +
theme_bw()
library(ENMeval)
?partitions
devtools::document()
library(ecospat)
devtools::document()
devtools::doc
devtools::document()
library(ecospat)
install.packages("ecospat")
library(ecospat)
devtools::document()
library(ENMeval)
?ENMevaluate
e=readRDS("/Volumes/bucket/EconomoU/GABI/10arcmin_v12_30km_15a_06292021/sdm_species/Acanthognathus.brevicornis/opt_model_seq.rds")
e
