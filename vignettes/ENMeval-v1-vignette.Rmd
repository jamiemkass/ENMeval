---
title: "ENMeval v1.0.0 Vignette"
author: "Jamie M. Kass, Robert Muscarella, and Peter Galante"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ENMeval v1.0.0 Vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: inline
---

```{r setup, echo = FALSE, include=FALSE}
library(knitr)
knitr::opts_chunk$set(collapse=TRUE, message=FALSE, warning=FALSE, comment="#>")
```

- [Introduction](#intro)
- [Data Acquisition & Pre-processing](#data)
- [Partitioning Occurrences for Evaluation](#partition)
- [Running ENMeval](#eval)
- [Plotting results](#plot)
- [Downstream Analyses](#downstream)
- [Resources](#resources)


## Introduction {#intro}

[`ENMeval`](https://cran.r-project.org/package=ENMeval) is an R package that performs automated runs and evaluations of ecological niche models (ENMs, a.k.a. SDMs), which can estimate species' ranges and niche characteristics from data on species occurrences and environmental variables. 

Some of the most frequently used ENMs are machine learning algorithms with settings that can be "tuned" to determine optimal levels of model complexity. In implementation, this means building models of varying settings, then evaluating them and comparing their performance to select the optimal settings. Such tuning exercises can result in models that balance goodness-of-fit (i.e., avoiding overfitting) and predictive ability. Model evaluation is often done with cross validation, which consists of partitioning the data into groups, building a model with all the groups but one, evaluating this model on the left-out group, then repeating the process until all groups have been left out once.

The primary function, `ENMevaluate`, does all the heavy lifting and returns several items including a table of evaluation statistics and, for each setting combination (here, colloquially: *runs*), a model object and a raster layer showing the model prediction across the study extent. There are also options for calculating niche overlap between predictions, running in parallel to speed up computation, and more. For a more detailed description of the package, check out the open-access publication:

[Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods in Ecology and Evolution, 5: 1198–1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

Older versions (v0.3.0 and earlier) implemented only [Maxent](http://biodiversityinformatics.amnh.org/open_source/maxent/) and [maxnet](https://cran.r-project.org/package=maxnet), but v1.0.0 and onward can implement any ENM in theory provided its settings are specified in a ENMdetails object (see below for an example). This version comes out of the box with Maxent, maxnet, [BIOCLIM](https://onlinelibrary.wiley.com/doi/full/10.1111/ddi.12144), and [BRT](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2656.2008.01390.x) (boosted regression trees), but users can provide their own custom model specifications. 

## Data Acquisition & Pre-processing {#data}
In this vignette, we briefly demonstrate acquisition and pre-processing of input data for `ENMeval`. There are a number of other excellent tutorials on these steps, some of which we compiled in the [Resources](#resources) section.

We'll start by downloading an occurrence dataset for [*Bradypus variegatus*](https://en.wikipedia.org/wiki/Brown-throated_sloth), the Brown-throated sloth.  We'll go ahead and load the `ENMeval`, `dplyr` for data management, and [`spocc`](https://cran.r-project.org/package=spocc) packages (which we use to download occurrence records).

```{r occDownload}
library(spocc)
library(dplyr)
library(ENMeval)

# set a random seed for reproducibility
set.seed(48)

# Search GBIF for occurrence data.
bv <- occ('Bradypus variegatus', 'gbif', limit=300, has_coords=TRUE)

# Get the latitude/coordinates for each locality. Also convert the tibble that occ() outputs
# to a data frame for compatibility with ENMeval functions.
occs <- as.data.frame(bv$gbif$data$Bradypus_variegatus[,2:3])

# Remove duplicate rows (Note that you may or may not want to do this).
occs <- occs[!duplicated(occs),]
```

We are going to model the climatic niche suitability for our focal species using climate data from [WorldClim](http://www.worldclim.org/). WorldClim has a range of variables available at various resolutions; for simplicity, here we'll use the 9 bioclimatic variables at 10 arcmin resolution (about 20 km across at the equator) included in the `dismo` package. These climatic data are based on 50-year averages from 1950-2000. Now's also a good time to load the package, as it includes all the downstream dependencies (`raster`, `dismo`, etc.).

```{r envDownload, warning=FALSE, message=FALSE, fig.width = 5, fig.height = 5}
library(raster)

# Locate some predictor raster files from the dismo folder
files <- list.files(path=paste(system.file(package='dismo'), '/ex', sep=''), pattern='grd', full.names=TRUE)

# Read the raster files into a RasterStack
envs <- stack(files)

# Plot first raster in the stack, bio1
plot(envs[[1]], main=names(envs)[1])

# Add points for all the occurrence points onto the raster
points(occs)

# There are some points east on the Amazon. Let's say we know that this represents a subpopulation that we don't want to include in the model -- we can remove these points from the analysis by subsetting the occurrences by latitude and longitude
occs <- filter(occs, latitude > -20, longitude < -45)

# Plot the subsetted occurrences over the original ones to make sure we excluded the right ones
points(occs, col='red')

# As we will demonstrate model evaluation using independent data, we will specify a fake independent occurrence dataset
occs.ind <- data.frame(longitude = -runif(10, 55, 65), latitude = runif(10, 0, 5))
points(occs.ind, col="blue")
```

Next, we will specify the background extent by cropping (or "clipping" in ArcGIS terms) our global predictor variable rasters to a smaller region. Since our models will compare the environment at occurrence (i.e., presence) localities to the environment at background localities, we need to sample random points from a background extent. To help ensure we don't include areas that are suitable for our species but are unoccupied due to limitations like dispersal constraints, we will conservatively define the background extent as an area surrounding our occurrence localities. We will do this by buffering a bounding box that includes all occurrence localities. Some other methods of background extent delineation (e.g., minimum convex hulls) are more conservative because they better characterize the geographic space holding the points. In any case, this is one of the many things that you will need to carefully consider for your own study.

```{r backgExt, message=FALSE, warning=FALSE}
library(sf)
library(sp)

# Make a SpatialPoints object
occs.sf <- st_as_sf(occs, coords = c("longitude","latitude"), crs = 4326)
# occs.sf <- st_transform(occs.sf, "+proj=cea +lat_ts=0 +lon_0=-65.302734375")

# Buffer all occurrences by 5 degrees
# * this will give a warning because we are not buffering in a projected coordinate system
# * for simplicity, this vignette does not involve coordinate reference system (CRS) transformations,
# * but for a real analysis, transforming to a projected CRS before buffering is best practice
occs.buf <- st_buffer(occs.sf, dist = 5)
plot(envs[[1]], main=names(envs)[1])
plot(occs.buf, add = TRUE)

# Crop environmental rasters to match the study extent
envs.bg <- crop(envs, occs.buf)
# Next, mask the rasters to the shape of the buffers
envs.bg <- mask(envs.bg, occs.buf)
```

We may want to restrict the study extent to the continental areas (for example). We can use the simplified world map from the [`maptools`](https://cran.r-project.org/package=maptools) package, which is not automatically loaded with `ENMeval`, and tools from `sf` to exclude the Caribbean from our study extent.

```{r removeCaribbean, message=FALSE, fig.width = 5, fig.height = 5, warning = FALSE}
library(maptools)

# Get a simple world countries polygon
data(wrld_simpl)

# Get polygons for Central and South America
central.amer <- wrld_simpl@data$SUBREGION==5
south.amer <- wrld_simpl@data$SUBREGION==13
# Subset wrld_simpl and convert to sf
ca.sa <- st_as_sf(wrld_simpl[central.amer | south.amer,])

# Buffer by 1 degree before masking to make sure you don't clip the coastline
ca.sa.buf <- st_buffer(ca.sa, dist = 1)

# Mask envs by the combined Central-South America polygon excluding the Caribbean
envs.bg <- mask(envs.bg, ca.sa.buf)

# Plot one of the new masked rasters -- we still have a few cells from the Galapagos, but are now missing the Caribbean
plot(envs.bg[[1]], main=names(envs.bg)[1])
points(occs)
```

In the next step, we'll sample 10,000 random points from the background (note that the number of background points is also a consideration you should make with respect to your own study).

```{r backgPts, fig.width = 5, fig.height = 5}
library(dismo)

# Randomly sample 10,000 background points from one background extent raster (only one per cell without replacement). Note: Since the raster has <10,000 pixels, you'll get a warning and all pixels will be used for background. We will be sampling from the biome variable because it is missing some grid cells, and we are trying to avoid getting background points with NA.
bg <- randomPoints(envs.bg[[9]], n=10000)
bg <- as.data.frame(bg)
colnames(bg) <- colnames(occs)

# Notice how we have pretty good coverage (every cell).
plot(envs.bg[[1]], legend=FALSE)
points(bg, col='red')
```

## Partitioning Occurrences for Evaluation {#partition}
A run of ENMevaluate begins by using one of six methods to partition occurrence localities into testing and training bins (folds) for *k*-fold cross-validation (Fielding and Bell 1997; Peterson et al. 2011). Data partitioning is done internally by `ENMevaluate()`, but can also be done externally with the partitioning functions. In this section, we explain and illustrate these different functions.

1. [Spatial Block](#block)
2. [Spatial Checkerboard](#cb1)
3. [Spatial Hierarchical Checkerboard](#cb2)
4. [Jackknife (leave-one-out)](#jack)
5. [Random *k*-fold](#rand)
6. [Independent](#ind)
7. [User](#user)

The first three partitioning methods are variations of what Radosavljevic and Anderson (2014) referred to as 'masked geographically structured' data partitioning. Basically, these methods partition both occurrence records and background points into evaluation bins based on some spatial rules. The intention is to reduce spatial-autocorrelation between points that are included in the testing and training bins, which can overinflate model performance, at least for data sets that result from biased sampling (Veloz 2009; Hijmans 2012; Wenger and Olden 2012).

#### 1. Block {#block}
First, the 'block' method partitions data according to the latitude and longitude lines that divide the occurrence localities into four spatial groups of (insofar as possible) equal numbers. Both occurrence and background localities are assigned to each of the four bins based on their position with respect to these lines. The resulting object is a list of two vectors that supply the bin designation for each occurrence and background point.

```{r part.block, fig.width = 5, fig.height = 5}
block <- get.block(occs, bg)
plot.grps(pts = occs, pts.grp = block$occ.grp, envs = envs.bg)
# PLotting the background shows that the background extent is partitioned in a way that maximizes evenness
# of points across the four bins, not to maximize evenness of area
plot.grps(pts = bg, pts.grp = block$bg.grp, envs = envs.bg)
```

#### 2. Checkerboard1 {#cb1}
The next two partitioning methods are variants of a 'checkerboard' approach to partition occurrence localities. These generate checkerboard grids across the study extent and partition the localities into groups based on where they fall on the checkerboard. In contrast to the block method, both checkerboard methods subdivide geographic space equally but do not ensure a balanced number of occurrence localities in each bin. For these methods, the user needs to provide a raster layer on which to base the underlying checkerboard pattern. Here we simply use the predictor variable RasterStack. Additionally, the user needs to define an *aggregation.factor*. This value tells the number of grids cells to aggregate when making the underlying checkerboard pattern.

The Checkerboard1 method partitions the points into *k* = 2 spatial groups using a simple checkerboard pattern.

```{r part.ck1, fig.width = 5, fig.height = 5}
cb1 <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=5)
plot.grps(pts = occs, pts.grp = cb1$occ.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
plot.grps(pts = bg, pts.grp = cb1$bg.grp, envs = envs.bg)

# We can increase the aggregation factor to give the groups bigger boxes
cb1.large <- get.checkerboard1(occs, envs.bg, bg, aggregation.factor=30)
plot.grps(pts = occs, pts.grp = cb1.large$occ.grp, envs = envs.bg)
plot.grps(pts = bg, pts.grp = cb1.large$bg.grp, envs = envs.bg)
```

#### 3. Checkerboard2 {#cb2}
The Checkerboard2 method partitions the data into *k* = 4 spatial groups. This is done by hierarchically aggregating the input raster at two scales. Presence and background groups are assigned based on which box they fall on the hierarchical checkerboard.

```{r part.ck2, fig.width = 5, fig.height = 5}
cb2 <- get.checkerboard2(occs, envs.bg, bg, aggregation.factor=c(5,5))
plot.grps(pts = occs, pts.grp = cb2$occ.grp, envs = envs.bg)
# Plotting the background points shows the checkerboard pattern very clearly
plot.grps(pts = bg, pts.grp = cb2$bg.grp, envs = envs.bg)
```

#### 4. Jackknife (leave-one-out) {#jack}
The next two methods differ from the first three in that (i) they do not partition the background points into different groups, and (ii) they do not account for spatial autocorrelation between testing and training localities. Primarily when working with relatively small data sets (e.g. < ca. 25 presence localities), users may choose a special case of *k*-fold cross-validation where the number of bins (*k*) is equal to the number of occurrence localities (*n*) in the data set (Pearson et al. 2007; Shcheglovitova and Anderson 2013). This is referred to as jackknife, or leave-one-out, partitioning.  As *n* models are processed with this partitioning method, the computation time could be long for large occurrence datasets. The background is not partitioned with jackknife.

```{r part.jk, fig.width = 5, fig.height = 5}
jack <- get.jackknife(occs, bg)
# If the number of input points is larger than 10, the legend for the groups is suppressed
plot.grps(pts = occs, pts.grp = jack$occ.grp, envs = envs.bg)
```

#### 5. Random k-fold {#rand}
The 'random k-fold' method partitions occurrence localities randomly into a user specified number of (*k*) bins. This method is equivalent to the 'cross-validate' partitioning scheme available in the current version of the Maxent software GUI. Especially with larger occurrence datasets, this partitioning method could randomly result in some spatial clustering of groups, which is why spatial partitioning methods are preferable for addressing spatial autocorrelation. Below, we partition the data into five random groups. The background is not partitioned with random k-fold.

```{r part.rand, fig.width = 5, fig.height = 5}
rand <- get.randomkfold(occs, bg, k = 5)
plot.grps(pts = occs, pts.grp = rand$occ.grp, envs = envs.bg)
```

#### 6. Independent {#ind}: 
The 'independent' method evaluates the model on an independent dataset that is not used to create the full model, and thus cross validation statistics are not calculated. To illustrate this, we will make a table containing occurrences representing both training data and independent testing data and plot the partitions in the same way as above. However, unlike before, the independent testing data (group 2) will not become training data for a new model. Instead, the training data (group 1) is used to make the model, and the independent testing data (group 2) is used only to evaluate it. Thus, the background extent does not include the independent testing data (a few points fall inside this extent because of the buffer we applied, but they are not used as training data).

```{r part.ind, fig.width = 5, fig.height = 5}
pts <- rbind(occs, occs.ind)
pts.grp <- c(rep(1, nrow(occs)), rep(2, nrow(occs.ind)))
plot.grps(pts = pts, pts.grp = pts.grp, envs = envs.bg)
```

#### 7. User-defined {#user}
For maximum flexibility, the last partitioning method is designed so that users can define *a priori* partitions. This provides a flexible way to conduct spatially-independent cross-validation with background masking. For example, we demonstrate partitioning the occurrence data based on *k*-means groups. The user-defined partition option can also be used to input partition groups derived from other sources, such as the [`blockCV`](https://github.com/rvalavi/blockCV) package available on Github.

```{r part.user1, fig.width = 5, fig.height = 5}
grp.n <- 6
kmeans <- kmeans(occs, grp.n)
occ.grp <- kmeans$cluster
plot.grps(pts = occs, pts.grp = occ.grp, envs = envs.bg)
```

When using the user-defined partitioning method, we need to supply ENMevaluate with group identifiers for both occurrence points AND background points. If we want to use all background points for each group, we can set the background to zero.

```{r part.user2, fig.width = 5, fig.height = 5}
bg.grp <- rep(0, nrow(bg))
plot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Alternatively, we may think of various ways to partition background data. This depends on the goals of the study but we might, for example, find it reasonable to partition background by clustering around the centroids of the occurrence clusters.

```{r part.user3, fig.width = 5, fig.height = 5}
centers <- kmeans$center
d <- pointDistance(bg, centers, lonlat=T)
bg.grp <- apply(d, 1, function(x) which(x==min(x)))
plot.grps(pts = bg, pts.grp = bg.grp, envs = envs.bg)
```

Choosing among these data partitioning methods depends on the research objectives and the characteristics of the study system. Refer to the [Resources](Resources) section for additional considerations on appropriate partitioning for evaluation.

## Running ENMeval {#eval}
Once you decide which method of data partitioning you would like to use, you are ready to start building models. We now move on to the main function in ENMeval: `ENMevaluate`.

- [Initial considerations](#eval.consid)
- [Different parameterizations](#eval.parameterizations)
- [Exploring the results (the ENMevaluate object)](#eval.explore)

#### Initial considerations {#eval.consid}
The two main parameters to define when calling `ENMevaluate` are (1) the range of regularization multiplier values and (2) the combinations of feature class to consider. The ***regularization multiplier*** (RM) determines the penalty for adding parameters to the model. Higher RM values impose a stronger penalty on model complexity and thus result in simpler (*flatter*) model predictions. The ***feature classes*** determine the potential shape of the response curves. A model that is only allowed to include linear feature classes will most likely be simpler than a model that is allowed to include all possible feature classes. Much more description of these parameters is available in the [Resources](#resources) section. For the purposes of this vignette, we demonstrate simply how to adjust these parameters. The following section deals with comparing the outputs of each model.

Unless you supply the function with background points (which is recommended in many cases), you will need to define how many background points should be used with the 'n.bg' argument. If any of your predictor variables are categorical (e.g., biomes), you will need to define which layer(s) these are using the 'categoricals' argument.

ENMevaluate builds a separate model for each unique combination of RM values and feature class combinations. For example, the following call will build and evaluate 2 models. One with RM=1 and another with RM=2, both allowing only linear features.

<!-- ```{r load_vignette_data, echo = FALSE} -->
<!-- data(eval2) -->
<!-- ``` -->

```{r enmeval1a, eval=FALSE}
eval1 <- ENMevaluate(occ=occs, env=envs, bg.coords=bg, partitions='checkerboard2', 
                     tune.args=list(fc="L", rm=1:2), mod.name='maxnet')
```

We may, however, want to compare a wider range of models that can use a wider variety of feature classes and regularization multipliers:

```{r enmeval1b, eval=TRUE}
eval2 <- ENMevaluate(occ=occs, env=envs, bg.coords=bg, partitions='checkerboard2', 
                     tune.args=list(fc=c("L","LQ","LQP"), rm=1:3), mod.name='maxnet')
```

When building many models, the command may take a long time to run. Of course this depends on the size of your dataset and the computer you are using. When working on big projects, running the command in parallel (`parallel=T`) can be faster. Note that running parallel can also be slower when working on small projects...

```{r enmeval2par, eval=FALSE}
eval2.par <- ENMevaluate(occ=occs, env=envs, bg.coords=bg, partitions='checkerboard2', 
                         tune.args=list(fc=c("L","LQ","LQP"), rm=1:2), mod.name='maxnet', parallel=TRUE)
```

Another way to save time at this stage is to turn off the option that generates model predictions across the full study extent (`rasterPreds=F`). Note, however, that the full model predictions are needed for calculating AICc values so those are returned as NA in the results table when the `rasterPreds` argument is set to NULL.

```{r enmeval3, eval=FALSE}
eval3 <- ENMevaluate(occ=occs, env=envs, bg.coords=bg, partitions='checkerboard2', 
                         tune.args=list(fc=c("L","LQ","LQP"), rm=1:2), mod.name='maxnet', rasterPreds=NULL)
```

We can also calculate one of two niche overlap statistics while running `ENMevaluate` by setting the arguments `overlap=T` and `overlapStat`, which support Moran's I or Schoener's D. Note that you can also calculate this value at a later stage using the separate `calc.niche.overlap` function.

```{r enmeval4, results='hide'}
overlap <- calc.niche.overlap(eval2@predictions, overlapStat="D")
overlap
```

#### Different parameterizations {#eval.parameterizations}
There are multiple ways to run the function `ENMevaluate()`, and we will go over how to specify each parameterization and what the effects are on the results.

full.mod.pred: raster model prediction based on `envs`
cbi: a) calculated on raster model prediction based on `envs`, b) cbi.test cannot only be calculated for partitions "randomkfold", "independent", or "user"


```{r evalExamples, eval= F}
# This example is for Maxent models, and so we will specify the tune.tbl with ranges of feature classes and regularization multipliers
tune.args <- list(fc = c("L", "LQ", "H", "LQH"), rm = 1:5)

# 1. Standard
e <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "block")

# 2. Independent partition: no cross validation statistics calculated; instead, model will be evaluated on an independent dataset that is not used to create the full model
e <- ENMevaluate(occs[1:250,], envs, bg, mod.name = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "independent", occs.ind = occs[251:nrow(occs),])

# 3. User partitions
user.grp <- list(occ.grp = round(runif(nrow(occs), 1, 2)), bg.grp = round(runif(nrow(bg), 1, 2)))
e <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "user", user.grp = user.grp)

# 4. No partitions: no cross validation statistics calculated, nor any model evaluation on test data
e <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "none")

# 5. No raster data (a.k.a, samples with data, or SWD): no full model raster predictions created, so will run faster; also, both cbi.train and cbi.test will be calculated on the point data (training and testing background) instead of on the "envs" rasters (default)
occs.vals <- cbind(occs, raster::extract(envs, occs))
bg.vals <- cbind(bg, raster::extract(envs, bg))
e <- ENMevaluate(occs.vals, bg = bg.vals, mod.name = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "block")

# 6. Background test CBI: this will calculate cbi.test on the point data (training and testing background) instead of on the "envs" rasters (default)
e <- ENMevaluate(occs, envs, bg, mod.name = "maxnet", tune.args = tune.args, categoricals = "biome", partitions = "block", cbi.eval = "bg")

# 7. User model: models will be fit and evaluated with the user-specified model and associated settings 
mod1 <- ENMdetails(...)
e <- ENMevaluate(occs, envs, bg, user.enm = mod1, tune.args = tune.args, categoricals = "biome", partitions = "block")
```


<!-- The `bin.output` argument determines if separate evaluation statistics for each testing bin are included in the results file.  If `bin.output=FALSE`, only the mean and variance of evaluation statistics across k bins is returned. -->

#### Exploring the results {#eval.explore}
Now let's take a look at the output from `ENMevaluate` (which is an object of class `ENMevaluation`) in more detail (also see `?ENMevaluation`).  It contains the following slots:

- `algorithm` A character vector showing which algorithm was used
- `tune.settings` A data.frame of settings that were tuned
- `partition.method` A character of partitioning method used
- `results` A data.frame of evaluation summary statistics
- `results.grp` A data.frame of evaluation k-fold statistics
- `models` A list of model objects
- `predictions` A RasterStack of model predictions
- `occ.pts` A data.frame of occurrence coordinates used for model training
- `occ.grp` A vector of partition groups for occurrence points
- `bg.pts` A data.frame of background coordinates used for model training
- `bg.grp` A vector of partition groups for background points
- `overlap` A list of matrices of pairwise niche overlap statistics

Let's first examine the structure of the object:
```{r results1}
eval2

str(eval2, max.level=3)
```

The first slot tells which algorithm was used (maxent.jar or maxnet) and which version of the software.
```{r algorithm}
eval2@algorithm
```

The next two slots hold information on which tuning settings and partitioning method were used.  The details of the tuning settings will depend on the algorithm used.
```{r settings}
eval2@tune.settings
eval2@partition.method
```

The original data (occurrence and background points), as well as which of the k-fold groups they belong to, are all held in `ENMevaluation` object.
```{r groups}
eval2@occs
eval2@occ.grp
eval2@bg
eval2@bg.grp
```

The next two slots hold the overall table of evaluation metrics and then the results for each k-fold group.  We can use these to, for example, select the 'best' model based on one or more of our evaluation criteria.  Let's find the model settings that resulted in the lowest AICc.
```{r evaluations}
# Overall results
eval2@results
eval2@results[which(eval2@results$delta.AICc==0),]
# Results for individual k-fold groups
eval2@results.grp
```

If we used the 'maxent.jar' algorithm, we can also access a list of Maxent model objects, which (as all lists) can be subset with double brackets (e.g. `results@eval2[[1]]`). The Maxent model objects provide access to various elements of the model (including the lambda file). The model objects can also be used for predicting models into other time periods or geographic areas. Note that the html file that is created when Maxent is run is **not** kept.

The "lambdas" slot for maxent.jar models shows which variables were included in the model. After the variable name, the next number is the variable coefficient, then the min and max of that variable for the inut data. If the coefficient is 0, that variable was not included in the model. Information regarding the kind of variable (e.g., quadratic, product) is included using symbolic notation, but you will likely find the syntax to be cryptic and the information is not stored in a very user-friendly way. The `parse_lambdas()` function in the [`rmaxent`](https://github.com/johnbaums/rmaxent/blob/master/) package parses this file into a more easily understandable data.frame.

The "betas" slot in a maxnet model is a named vector of the variable coefficients and what kind they are (in R formula notation).

Let's look at the model object for our "AICc optimal" model:
```{r mod.obj1}
aic.mod.name <- eval2@results %>% filter(delta.AICc == 0) %>% pull(tune.args)
aic.opt <- eval2@models[[aic.mod.name]]
#aic.opt
```

The "results" slot shows the Maxent model statistics:
```{r mod.obj3}
# For maxent.jar: model statistics
eval2@results[aic.mod.name,]
# For maxent.jar: lambdas file (shows which model parameters were used for training)
aic.opt$lambda
rmaxent::parse_lambdas(aic.opt)
# For maxnet: betas (shows which model parameters were used for training)
aic.opt$betas
```

Now let's access the RasterStack of the model predictions. Note that by default for maxent.jar or maxnet models, these predictions are in the 'cloglog' output format that is bounded between 0 and 1. This can be changed with the pred.type parameter in `ENMevaluate()`.
```{r preds}
eval2@predictions
```

Now plot the model with the lowest AICc. These predictions are for the entire extent of the input predictor variable rasters, and thus include areas outside of the background extent used for model training. Thus, we should interpret areas far outside this extent with caution.
```{r plotMod, fig.width = 5, fig.height = 5}
plot(eval2@predictions[[aic.mod.name]])
```

<!-- You can use the `var.importance` function to get a data.frame of two variable importance metrics: percent contribution and permutation importance.  See the function help file for more information on these metrics. -->
<!-- ```{r mod.obj4} -->
<!-- var.importance(aic.opt) -->
<!-- ``` -->

## Plotting results {#plot}
Plotting options in R are extremely flexible and here we demonstrate some key tools to explore the results of an ENMevaluate object graphically.

- [Plotting model predictions](#plot.preds)
- [Plotting response curves](#plot.resp)

ENMeval has a built-in plotting function (`eval.plot`) to visualize the results of different models.  It requires the ENMevaluation object.  By default, it always plots delta.AICc values. Here, we will plot AICc and delta.AICc values for each feature class for both regularization multipliers.

```{r plot.res, fig.width = 5, fig.height = 5}
plot.eval(e = eval2, stats = "delta.AICc", col = "fc", x = "rm")
```

You can choose which evaluation metric to plot. Cross-validation metrics are plotted as averages with standard deviation error bars, and the points are jittered by default for ease of interpretation (this value can be changed with the parameter "dodge"). Error bars can be turned off by setting the parameter "error.bars" to FALSE.

```{r plot.res2, fig.width = 5, fig.height = 5}
plot.eval(eval2, stats = 'auc.train', col = 'fc', x = "rm")
plot.eval(eval2, stats = 'auc.test', col = 'fc', x = "rm")
plot.eval(eval2, stats = 'auc.test', col = 'fc', x = "rm", error.bars = FALSE)
```

<!-- You can also plot the permutation importance or percent contribution. -->
<!-- ``` {r plot.res3, fig.width = 5, fig.height = 5} -->
<!-- df <- var.importance(aic.opt) -->
<!-- barplot(df$permutation.importance, names.arg=df$variable, las=2, ylab="Permutation Importance") -->
<!-- ``` -->

#### Plotting model predictions {#plot.preds}
If you generated raster predictions of the models (i.e., `rasterpreds=T`), you can easily plot them. For example, let's look at the first two models included in our analysis - remember that the output values are in Maxent's 'raw' units.  

```{r plot.pred1, fig.width = 5, fig.height = 5, mar=c(2,2,1,0)}
plot(eval2@predictions[[1]], legend=F)

# Now add the occurrence and background points, colored by evaluation bins:
points(eval2@bg, pch=3, col=eval2@bg.grp, cex=0.5)
points(eval2@occs, pch=21, bg=eval2@occ.grp)
```

Let's see how model complexity changes the predictions in our example.  We'll compare the model predictions of the model with only linear feature classes and with the highest regularization multiplier value we used (i.e., fc='L', RM=2) versus the model with all feature class combination and the lowest regularization multiplier value we used (i.e., fc='LQP',  RM=1).

```{r plot.pred2, fig.width = 5, fig.height = 2.5}
# bisect the plotting area to make two columns
par(mfrow=c(1,2), mar=c(2,2,1,0))

plot(eval2@predictions[['L_2']], ylim=c(-30,20), xlim=c(-90,-40), legend=F, main='L_2 prediction')

plot(eval2@predictions[['LQP_1']], ylim=c(-30,20), xlim=c(-90,-40), legend=F, main='LQP_1 prediction')
```

#### Plotting response curves {#plot.resp}
We can also plot the response curves of our model to see how different input variables influence our model predictions.  (Note that, as with the `dismo::maxent` function, using this function requires that the maxent.jar file be installed in the `dismo` package java folder).

```{r response_curves, eval=FALSE}
response(eval2@models[[1]])
```

## Downstream Analyses (*under construction*) {#downstream}
Below is a running list of other things we plan to add to this vignette.  Feel free to let us know if there are particular things you would like to see added.

- Working with the output from the 'maxnet' algorithm
- Extracting model results from object (various thresholds)
- Use model object to make a new prediction (e.g., if you want a logistic prediction)
- Make a projection to a new extent
- Do MESS map (Use `mess()` in the `dismo` package)

## Resources (*under construction*) {#resources}

###### Web Resources
- [Hijmans, R. and Elith, J. (2016) Species distribution modeling with R. dismo vignette.](https://cran.r-project.org/package=dismo)

- [Phillips, S. J. (2006) Phillips, S. (2006) A brief tutorial on Maxent. AT&T Research. Available at: http://biodiversityinformatics.amnh.org/open_source/maxent/](http://biodiversityinformatics.amnh.org/open_source/maxent/)

- [Yoder, J. (2013) Species distribution models in R. The Molecular Ecologist.](http://www.molecularecologist.com/2013/04/species-distribution-models-in-r/)

- [Maxent Google Group](https://groups.google.com/forum/embed/#!forum/maxent)

###### General Guides
- [Merow, C., Smith, M., and Silander, J.A. (2013) A practical guide to Maxent: what it does, and why inputs and settings matter. Ecography 36, 1-12.](http://onlinelibrary.wiley.com/doi/10.1111/j.1600-0587.2013.07872.x/abstract)

- [Peterson, A.T., Soberón, J., Pearson, R.G., Anderson, R.P., Martínez-Meyer, E., Nakamura, M., and Araújo, M.B. (2011) Ecological Niches and Geographic Distributions. Monographs in Population Biology, 49. Princeton University Press.](http://press.princeton.edu/titles/9641.html)

- [Renner, I.W., Elith, J., Baddeley, A., Fithian, W., Hastie, T., Phillips, S.J., . . . Warton, D.I. (2015) Point process models for presence-only analysis. Methods in Ecology and Evolution 6, 366-379.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12352/abstract)

###### Model Evaluation
- [Aiello-Lammens, M.E., Boria, R.A., Radosavljevic, A., Vilela, B., and Anderson, R.P. (2015) spThin: an R package for spatial thinning of species occurrence records for use in ecological niche models. Ecography 38, 541-545.](http://onlinelibrary.wiley.com/doi/10.1111/ecog.01132/abstract)

- [Fielding, A.H. and Bell, J.F. (1997) A review of methods for the assessment of prediction errors in conservation presence-absence models. Environmental Conservation 24, 38-49.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.359&rep=rep1&type=pdf)

- [Hijmans, R.J. (2012) Cross-validation of species distribution models: removing spatial sorting bias and calibration with a null model. Ecology 93, 679-688.](http://onlinelibrary.wiley.com/doi/10.1890/11-0826.1/abstract)

- [Muscarella, R., Galante, P. J., Soley-Guardia, M., Boria, R. A., Kass, J. M., Uriarte, M. and Anderson, R. P. (2014), ENMeval: An R package for conducting spatially independent evaluations and estimating optimal model complexity for Maxent ecological niche models. Methods Ecol Evol, 5: 1198–1205.](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12261/full)

- [Radosavljevic, A. and Anderson, R.P. (2014) Making better Maxent models of species distributions: complexity, overfitting and evaluation. Journal of Biogeography 41, 629-643.](http://onlinelibrary.wiley.com/doi/10.1111/jbi.12227/abstract)

- [Shcheglovitova, M. and Anderson, R.P. (2013) Estimating optimal complexity for ecological niche models: A jackknife approach for species with small sample sizes. Ecol. Model. 269, 9-17.](http://www.sciencedirect.com/science/article/pii/S0304380013004043)

- [Veloz, S.D. (2009) Spatially autocorrelated sampling falsely inflates measures of accuracy for presence-only niche models. Journal of Biogeography 36, 2290-2299.](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2699.2009.02174.x/abstract)

- [Wenger, S.J. and Olden, J.D. (2012) Assessing transferability of ecological models: an underappreciated aspect of statistical validation. Methods in Ecology and Evolution 3, 260-267.](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00170.x/abstract)

###### Some Empirical Examples
- [Pearson, R.G., Raxworthy, C.J., Nakamura, M., and Peterson, A.T. (2007) Predicting species distributions from small numbers of occurrence records: a test case using cryptic geckos in Madagascar. Journal of Biogeography 34, 102-117.](http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2699.2006.01594.x/abstract)


